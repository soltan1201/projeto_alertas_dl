#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PASSO 2: Extração de Patches para TFRecord
Lê os pontos gerados no Passo 1 e extrai os pixels das imagens Sentinel-2.
"""
import logging
import sys
import math
import ee

# Ajuste o caminho das suas libs locais
pathparent = str('/home/superuser/Dados/projAlertas/proj_alertas_ML/src')
sys.path.append(pathparent)
from configure_account_projects_ee import get_current_account

projAccount = get_current_account()
print(f"Projeto selecionado >>> {projAccount} <<<")

try:
    ee.Initialize(project=projAccount)
except Exception as e:
    print("Erro de Inicialização:", e)
    raise

# ==============================================================================
# 1. CONFIGURAÇÕES
# ==============================================================================
# Onde estão os pontos gerados pelo seu script anterior?
ASSET_POINTS_FOLDER = 'projects/mapbiomas-caatinga-cloud04/assets/Alertas/featCol_samples/setembro'

# Onde salvar os TFRecords finais no Google Drive?
EXPORT_DRIVE_FOLDER = 'DATASET_CHANGE_DETECTION_S2_TFRECORDS'

# Onde estão os alertas originais (para pintar o Label Raster)?
ASSET_ALERTS_POLY = 'projects/mapbiomas-caatinga-cloud04/assets/Alertas/revisados_2025/alerts_pol_setembro_2025-09-01'

# Parâmetros
month_name = 'setembro'
PATCH_SIZE = 256
SCALE = 10
MAX_PATCHES_PER_FILE = 100 

# Datas
DATA_INIC = '2025-09-01'
DATA_FIM = '2025-10-01'

# Bandas
BANDS_S2 = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
ALL_BANDS = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12', 'ndfia', 'ratio_brown', 'savi']

# ==============================================================================
# 2. FUNÇÕES DE PROCESSAMENTO DE IMAGEM
# ==============================================================================
def mask_clouds_s2_csplus(img):
    csPlus = ee.Image(img.get('cs_plus'))
    isClear = csPlus.select('cs').gte(0.60)
    return img.updateMask(isClear)

def add_indices_and_scale(img):
    img = mask_clouds_s2_csplus(img)
    # Escala para 0-1 para cálculos
    s2_scaled = img.select(BANDS_S2).divide(10000)
    
    # Endmembers (GV, NPV, Soil, Shade, Cloud)
    endmembers = [
        [0.05, 0.09, 0.04, 0.61, 0.30, 0.10], 
        [0.14, 0.17, 0.22, 0.30, 0.55, 0.30], 
        [0.20, 0.30, 0.34, 0.58, 0.60, 0.58], 
        [0.0,  0.0,  0.0,  0.0,  0.0,  0.0 ], 
        [0.90, 0.96, 0.80, 0.78, 0.72, 0.65]
    ]
    frac = s2_scaled.unmix(endmembers, True, True).rename(['gv','npv','soil','shade','cloud'])
    
    gv_shade = frac.select('gv').divide(frac.select('shade').subtract(1).abs())
    
    # Índices (Retornando para Int16 para economizar espaço no TFRecord)
    # NDFIa: (ndfia + 1) * 10000
    ndfia = (gv_shade.subtract(frac.select('soil').add(frac.select('npv'))) 
             .divide(gv_shade.add(frac.select('npv')).add(frac.select('soil')))
             .add(1).multiply(10000).toInt16().rename('ndfia'))
    
    ratio = (frac.select('npv').divide(frac.select('npv').add(frac.select('soil')))
             .multiply(10000).toInt16().rename('ratio_brown'))

    savi = (img.expression('((NIR - RED) * 1.5) / (NIR + RED + 0.5)', 
            {'NIR': img.select('B8'), 'RED': img.select('B4')})
            .add(1.5).multiply(10000).toInt16().rename('savi'))
    
    # Bandas originais voltam para Int16 (escala Sentinel padrão)
    return (s2_scaled.multiply(10000).toInt16()
            .addBands([ndfia, ratio, savi])
            .copyProperties(img, ['system:time_start']))

# ==============================================================================
# 3. PIPELINE DE EXTRAÇÃO
# ==============================================================================

# Grade Landsat (apenas para referência de geometria se necessário)
asset_grade = 'users/CartasSol/shapes/grade_landsat_tm_Br'
grades_fc = ee.FeatureCollection(asset_grade)

# Listar os assets de PONTOS gerados no Passo 1
print("Listando assets de pontos...")
asset_list = ee.data.listAssets(ASSET_POINTS_FOLDER)
# agora tem todos os alertas mais os verdadeiros
asset_list = ee.Dictionary(asset_list).get('assets').getInfo()
print(f"Total de Grids/Assets para processar: {len(asset_list)}")

# Loop principal
inic = 25
end = 50
for cc, asset_info in enumerate(asset_list[inic: end]):
    points_asset_id = asset_info['id']
    name_export = points_asset_id.split('/')[-1] # Ex: sample_215_64_setembro
    
    print(f"\n Processing [{inic + cc + 1}/{len(asset_list)}] >> {name_export}")
    
    # Extrair Path/Row do nome do arquivo
    parts = name_export.split('_')
    try:
        # Ajuste os índices conforme o nome exato que seu script anterior gerou
        # Se for "sample_215_64_setembro", então:
        wrs_path = parts[1] 
        wrs_row = parts[2]
    
    except:
        print(f"Skipping name format error: {name_export}")
        continue
    
    print(f"{wrs_path} / {wrs_row}")
    # 1. Definir Geometria da Grade (Clip Region)
    grade_geom = (grades_fc
                  .filter(ee.Filter.eq('ORBITA', int(wrs_path)))
                  .filter(ee.Filter.eq('PONTO', int(wrs_row)))
                  .geometry())
    
    # 2. Carregar os PONTOS já prontos
    sample_points = ee.FeatureCollection(points_asset_id)
    print(" samples points ", sample_points.size().getInfo())
    # Otimização: Se o asset estiver vazio (sem alertas na cena), pula
    # try:
    #     if sample_points.size().getInfo() == 0:
    #         print("Grid sem pontos. Pulando.")
    #         continue
    # except:
    #     continue

    # 3. Construir as Imagens (T0, T1, Label)
    # sys.exit()
    # --- T1 (Imagem Atual) ---
    s2 = (ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED") 
        .filterDate(DATA_INIC, DATA_FIM) 
        .filterBounds(grade_geom) 
        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 70)))
    
    csPlus = (ee.ImageCollection("GOOGLE/CLOUD_SCORE_PLUS/V1/S2_HARMONIZED")
        .filterDate(DATA_INIC, DATA_FIM) 
        .filterBounds(grade_geom))

    linked = ee.ImageCollection(ee.Join.saveFirst('cs_plus').apply(
        primary=s2, secondary=csPlus,
        condition=ee.Filter.equals(leftField='system:index', rightField='system:index')
    ))
    mosaicMonth = linked.map(add_indices_and_scale).median()

    # --- T0 (Imagem Passada - 15 meses antes) ---
    d_bef = ee.Date(DATA_INIC).advance(-2, 'year')
    s2_bef = (ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED") 
              .filterDate(d_bef.advance(-2, 'month'), d_bef.advance(3, 'month')) 
              .filterBounds(grade_geom) 
              .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 70)))
    
    csPlus_bef = (ee.ImageCollection("GOOGLE/CLOUD_SCORE_PLUS/V1/S2_HARMONIZED") 
                  .filterDate(d_bef.advance(-2, 'month'), d_bef.advance(3, 'month')) 
                  .filterBounds(grade_geom))

    linked_bef = ee.ImageCollection(ee.Join.saveFirst('cs_plus').apply(
        primary=s2_bef, secondary=csPlus_bef,
        condition=ee.Filter.equals(leftField='system:index', rightField='system:index')
    ))
    mosaicBefore = linked_bef.map(add_indices_and_scale).median()#.clip(grade_geom)

    # --- Label (Ground Truth Rasterizado) ---
    # Fundo = 0, Alerta = 1
    # Importante: Usamos 'paint' nos polígonos originais de alerta
    fc_alerts_raw = ee.FeatureCollection(ASSET_ALERTS_POLY).filter(ee.Filter.bounds(grade_geom))
    alertas_img = ee.Image(0).byte().paint(fc_alerts_raw, 1).rename('label')#.clip(grade_geom)
    
    # 4. Empilhar Tudo (Stack)
    t0_bands = [b + '_t0' for b in ALL_BANDS]
    t1_bands = [b + '_t1' for b in ALL_BANDS]
    
    # Converte tudo para Int16 para padronizar TFRecord
    full_stack = (mosaicBefore.select(ALL_BANDS, t0_bands)
                  .addBands(mosaicMonth.select(ALL_BANDS, t1_bands))
                  .addBands(alertas_img) 
                  .clip(grade_geom)
                  .toInt16())
    print("show bands of images ", full_stack.bandNames().getInfo())

    # 5. Extração com neighborhoodToArray
    # Como os pontos já existem, isso é rápido.
    kernel = ee.Kernel.rectangle(PATCH_SIZE//2, PATCH_SIZE//2, 'pixels')
    patches_array = full_stack.neighborhoodToArray(kernel)
    
    

    # 6. Exportação com Sharding (Divisão de Arquivos)
    sample_points = sample_points.randomColumn('shard_index', seed=1)
    
    # Estimativa de shards.
    # Assumindo 5000 pontos max por grid. 5000/300 = ~17 shards.
    # Se você quiser ser preciso, precisaria do size(), mas isso demora.
    # Vamos usar um loop fixo seguro. O filtro vazio não gera arquivo ou gera arquivo vazio rápido.
    NUM_SHARDS = 4
    
    selectors = t0_bands + t1_bands + ['label']
    print(f"   Iniciando exportação em {NUM_SHARDS} shards...")

    # shard_samples = patches_array.sampleRegions(
    #     collection=sample_points,
    #     scale= SCALE,
    #     geometries=False, # TFRecord não precisa de lat/lon, só dos pixels
    #     tileScale=16      # CRUCIAL para evitar estouro de memória com arrays
    # )
    
    # fname = f"tfrecord_{wrs_path}_{wrs_row}_{month_name}" 
    
    # task = ee.batch.Export.table.toDrive(
    #     collection= shard_samples,
    #     description= fname,
    #     folder= EXPORT_DRIVE_FOLDER,
    #     fileFormat= 'TFRecord',
    #     selectors= selectors
    # )
    
    # task.start()
    
    for i in range(NUM_SHARDS):
        min_val = i / NUM_SHARDS
        max_val = (i + 1) / NUM_SHARDS
        
        # Filtra a fatia
        shard_points = sample_points.filter(
            ee.Filter.And(
                ee.Filter.gte('shard_index', min_val),
                ee.Filter.lt('shard_index', max_val)
            )).limit(MAX_PATCHES_PER_FILE)
        print("número de points selecionados ", shard_points.size().getInfo())
        
        # 2. Aplicamos sampleRegions APENAS neste subconjunto
        # tileScale=16 ajuda a evitar 'User Memory Limit' ao processar arrays pesados
        try:
            shard_samples = patches_array.sampleRegions(
                collection=shard_points,
                scale=SCALE,
                geometries=False, # TFRecord não precisa de lat/lon, só dos pixels
                tileScale=16      # CRUCIAL para evitar estouro de memória com arrays
            )
            
            fname = f"tfrecord_{wrs_path}_{wrs_row}_part{i:03d}"
            
            task = ee.batch.Export.table.toDrive(
                collection=shard_samples,
                description=fname,
                folder=EXPORT_DRIVE_FOLDER,
                fileFormat='TFRecord',
                selectors=selectors
            )
            
            task.start()
            print(f"     -> Task enviada: {fname}")
            
        except Exception as e:
            print(f"     Erro ao configurar task {i}: {e}")
        
        # sys.exit()

print("Loop finalizado. Verifique a aba Tasks.")